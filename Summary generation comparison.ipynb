{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyMnvsnC5dou/3/OV0Go28c7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Import required libraries"],"metadata":{"id":"R7RFbekQdIOw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mAMcJq3QsY8K"},"outputs":[],"source":["!pip install transformers torch accelerate tensorflow-hub bert-tensorflow tensorflow tqdm bert-score sentence_transformers rank_bm25"]},{"cell_type":"code","source":["!pip install datasets"],"metadata":{"id":"RlUvqlHTDt4f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install rouge-score"],"metadata":{"id":"gHTt5J3hEF2J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install sacrebleu"],"metadata":{"id":"nvD2gadfETTl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM, Trainer, TrainingArguments, BertTokenizer, BertForSequenceClassification, MarianMTModel, MarianTokenizer, BertConfig\n","import torch\n","from transformers import BertTokenizer, BertForSequenceClassification, TFBertForSequenceClassification\n","from transformers import LlamaTokenizer, LlamaForCausalLM\n","from transformers import pipeline\n","from sentence_transformers import SentenceTransformer, util\n","from sklearn.utils.class_weight import compute_class_weight\n","from torch.utils.data import Dataset\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.utils import resample\n","from rank_bm25 import BM25Okapi\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from statistics import mean\n","import pandas as pd\n","import tensorflow_hub as hub\n","import tensorflow as tf\n","from datetime import datetime\n","from torch.utils.data import DataLoader\n","import re\n","import nltk\n","from nltk.corpus import wordnet\n","import random\n","from tqdm import tqdm\n","from datasets import load_metric\n","from bert_score import score as bert_score\n","import concurrent.futures\n","from nltk.tokenize import sent_tokenize\n","import matplotlib.pyplot as plt\n","import json\n","import numpy as np\n","from bert_score import score\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n"],"metadata":{"id":"Zp856AlCsZf0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import and clean complete posts"],"metadata":{"id":"44xLBzNqdLDg"}},{"cell_type":"code","source":["nltk.download('punkt')\n","dataset_path = \"/content/drive/My Drive/Diss_Dataset/dataset500_cleaned.csv\"\n","data = pd.read_csv(dataset_path)\n","data[\"userid\"] = data.iloc[:, 0]\n","data[\"posts\"] = data.iloc[:, 1]\n","data[\"label\"] = data.iloc[:, 2]\n","\n","data['posts'] = data['posts'].str.strip('[]').str.split(\"', '\")\n","data['posts'] = data['posts'].apply(lambda x: [post.strip(\"' \") for post in x])\n","\n","def clean_post(post):\n","    post = re.sub(r'\\*+', '', post)\n","    post = re.sub(r'\\s+', ' ', post)\n","    post = re.sub(r'&gt;', '', post)\n","    post = re.sub(r'[^\\x00-\\x7F]+', '', post)\n","    post = re.sub(r'\"[^\"]*\"', '', post)\n","    post = re.sub(r'\\([^)]*\\)', '', post)\n","    post = re.sub(r'[()]', '', post)\n","    post = re.sub(r'\\[[^\\]]*\\]', '', post)\n","    post = re.sub(r'[\\[\\]]', '', post)\n","    return post.strip().lower()\n","\n","data[\"clean_posts\"] = data[\"posts\"].apply(lambda posts: [clean_post(post) for post in posts])\n","data['combined_posts'] = data['clean_posts'].apply(lambda posts: ' '.join(posts))\n","\n","\n","print(data['combined_posts'][1])"],"metadata":{"id":"90vKuvY-sa51"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import highlights, user risk and supportiveness percentage"],"metadata":{"id":"p7EeTlD6dOcU"}},{"cell_type":"code","source":["top_5_path = \"/content/drive/My Drive/Diss_Dataset/top_5_sentences_per_user.csv\"\n","top_5_df = pd.read_csv(top_5_path)\n","\n","user_risk_supportiveness_path = \"/content/drive/My Drive/Diss_Dataset/user_risk_with_supportiveness.csv\"\n","risk_supportiveness_df = pd.read_csv(user_risk_supportiveness_path)"],"metadata":{"id":"Ba5q3Rv9sdP1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create meta-information"],"metadata":{"id":"acujU3kNdTmJ"}},{"cell_type":"code","source":["def create_meta_info(user_id, risk_df, top_5_df):\n","    risk_info = risk_df[risk_df['userid'] == user_id]\n","    top_5_info = top_5_df[top_5_df['userid'] == user_id]\n","\n","    if not risk_info.empty:\n","        risk_val = risk_info['risk_rating'].values[0]\n","        risk = \"Very High\" if risk_val == 5 else \"High\" if risk_val == 4 else \"Medium\" if risk_val == 3 else \"Low\" if risk_val == 2 else \"Very Low\" if risk_val == 1 else \"No\"\n","        supportiveness_ratio = risk_info['supportiveness_ratio'].values[0]\n","    else:\n","        risk = \"Unknown\"\n","        supportiveness_ratio = \"Unknown\"\n","\n","    top_5_sentences = ' '.join(top_5_info['sentence'].tolist())\n","    supportive_percentage = (supportiveness_ratio * 100).round(2)\n","\n","    meta_info = f\"The author indicates {risk} suicidal risk, with {supportive_percentage}% of their sentences supporting other users.\"\n","    return meta_info"],"metadata":{"id":"FrjYrOmu5zKc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Clear GPU RAM and initialise a model"],"metadata":{"id":"AqtpjCepdWQj"}},{"cell_type":"code","source":["import gc\n","torch.cuda.empty_cache()\n","gc.collect()"],"metadata":{"id":"9EmpC6q-ALGF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name1 = \"/content/drive/My Drive/Diss_Dataset/DPO7b\"\n","tokenizer1 = AutoTokenizer.from_pretrained(model_name1)\n","model1 = AutoModelForCausalLM.from_pretrained(model_name1, device_map=\"auto\")"],"metadata":{"id":"aartfLZVsrAK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name1 = \"/content/drive/My Drive/Diss_Dataset/Mistral7b\"\n","tokenizer1 = AutoTokenizer.from_pretrained(model_name1)\n","model1 = AutoModelForCausalLM.from_pretrained(model_name1, torch_dtype=torch.bfloat16, device_map=\"auto\")"],"metadata":{"id":"cMdUdHz55jGA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Prompts for Tulu-2-DPO-7b"],"metadata":{"id":"EpkZCBE8Oz3i"}},{"cell_type":"code","source":["Zero_prompt_model1 = \"\"\"\n","<|user|> In one paragraph, summarise the provided text from the author on Reddit:\\n\n","\n","Full input text: {posts}\\n\n","\n","<|assistant|> Summary:\\n\n","\n","\"\"\""],"metadata":{"id":"1VARFNpk8a7f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Zero_prompt_meta_model1 = \"\"\"\n","<|user|> In one paragraph, summarise the provided text from the author on Reddit:\\n\n","Full input text: {posts}\\n\n","Meta-Information: {meta_info}\\n\n","<|assistant|> Summary:\\n\n","\n","\"\"\""],"metadata":{"id":"_EkjaDVD5ycm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CoT_prompt_meta_model1 = \"\"\"\n","<|user|> As a mental health expert, your task is to summarise in one paragraph, directly from the provided input from the author on Reddit. When summarising, consier these aspects:\\n\n","Emotions: Evaluate expressed emotions, from sadness to intense psychological pain.\n","Cognitions: Explore the individual’s thoughts and perceptions about suicide, including the level and frequency of suicidal thoughts, intentions of suicide, and any existing plans.\n","Behavior and Motivation: Evaluate the user’s actions related to suicide, such as access to means and concrete plans. Consider their ability to handle difficult/stressful situations and the motivations behind their desire to die.\n","Interpersonal and Social Support: Investigate the individual’s social support or stable relationships, and understand their feelings toward significant others.\n","Mental Health-Related Issues: Consider psychiatric diagnoses associated with suicide such as schizophrenia, bipolar, anxiety, eating disorder, previous suicidal attempts, and others.\n","Additional Risk Factors: Consider other factors like socioeconomic and demographic factors, exposure to suicide behavior by others, chronic medical conditions, etc.\n","Supportive Nature: Take into account authors who are supporting other users in their writing.\n","\n","Input to be summarised: {posts}\\n\n","Meta-Information: {meta_info}\\n\n","<|assistant|> Summary:\\n\n","\n","\"\"\""],"metadata":{"id":"QH8JOmva4yUd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CoT_prompt_model1 = \"\"\"\n","<|user|> As a mental health expert, your task is to summarise in one paragraph, directly from the provided input from the author on Reddit. When summarising, consier these aspects:\\n\n","Emotions: Evaluate expressed emotions, from sadness to intense psychological pain.\n","Cognitions: Explore the individual’s thoughts and perceptions about suicide, including the level and frequency of suicidal thoughts, intentions of suicide, and any existing plans.\n","Behavior and Motivation: Evaluate the user’s actions related to suicide, such as access to means and concrete plans. Consider their ability to handle difficult/stressful situations and the motivations behind their desire to die.\n","Interpersonal and Social Support: Investigate the individual’s social support or stable relationships, and understand their feelings toward significant others.\n","Mental Health-Related Issues: Consider psychiatric diagnoses associated with suicide such as schizophrenia, bipolar, anxiety, eating disorder, previous suicidal attempts, and others.\n","Additional Risk Factors: Consider other factors like socioeconomic and demographic factors, exposure to suicide behavior by others, chronic medical conditions, etc.\n","Supportive Nature: Take into account authors who are supporting other users in their writing.\n","\n","Input to be summarised: {posts}\\n\n","\n","<|assistant|> Summary:\\n\n","\n","\"\"\""],"metadata":{"id":"ohm3wW138Sd2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Prompts for Mistral-7b-instruct-v0.3"],"metadata":{"id":"sEb2J9jZO0VA"}},{"cell_type":"code","source":["Zero_prompt_model1 = \"\"\"\n","[INST]\n","In one paragraph, summarise the provided text from the author on Reddit:\\n\n","#Full input text:\n","{posts}\\n\n","Summary: [/INST]\n","\n","\"\"\""],"metadata":{"id":"VOna0dRYO8YJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Zero_prompt_meta_model1 = \"\"\"\n","[INST]\n","In one paragraph, summarise the provided text from the author on Reddit:\\n\n","#Full input text:\n","{posts}\\n\n","#Meta-Information:\n","{meta_info}\\n\n","Summary: [/INST]\n","\n","\"\"\""],"metadata":{"id":"ddo5lbK3PEwv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CoT_prompt_meta_model1 = \"\"\"\n","[INST]\n","As a mental health expert, your task is to summarise in one paragraph, directly from the provided input from the author on Reddit. When summarising, consier these aspects:\\n\n","Emotions: Evaluate expressed emotions, from sadness to intense psychological pain.\n","Cognitions: Explore the individual’s thoughts and perceptions about suicide, including the level and frequency of suicidal thoughts, intentions of suicide, and any existing plans.\n","Behavior and Motivation: Evaluate the user’s actions related to suicide, such as access to means and concrete plans. Consider their ability to handle difficult/stressful situations and the motivations behind their desire to die.\n","Interpersonal and Social Support: Investigate the individual’s social support or stable relationships, and understand their feelings toward significant others.\n","Mental Health-Related Issues: Consider psychiatric diagnoses associated with suicide such as schizophrenia, bipolar, anxiety, eating disorder, previous suicidal attempts, and others.\n","Additional Risk Factors: Consider other factors like socioeconomic and demographic factors, exposure to suicide behavior by others, chronic medical conditions, etc.\n","Supportive Nature: Take into account authors who are supporting other users in their writing.\n","\n","#Input to be summarised:\n","{posts}\\n\n","#Meta-Information:\n","{meta_info}\\n\n","#Summary: [/INST]\n","\n","\"\"\""],"metadata":{"id":"tt29BMmIPF8A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CoT_prompt_model1 = \"\"\"\n","[INST]\n","As a mental health expert, your task is to summarise in one paragraph, directly from the provided input from the author on Reddit. When summarising, consier these aspects:\\n\n","Emotions: Evaluate expressed emotions, from sadness to intense psychological pain.\n","Cognitions: Explore the individual’s thoughts and perceptions about suicide, including the level and frequency of suicidal thoughts, intentions of suicide, and any existing plans.\n","Behavior and Motivation: Evaluate the user’s actions related to suicide, such as access to means and concrete plans. Consider their ability to handle difficult/stressful situations and the motivations behind their desire to die.\n","Interpersonal and Social Support: Investigate the individual’s social support or stable relationships, and understand their feelings toward significant others.\n","Mental Health-Related Issues: Consider psychiatric diagnoses associated with suicide such as schizophrenia, bipolar, anxiety, eating disorder, previous suicidal attempts, and others.\n","Additional Risk Factors: Consider other factors like socioeconomic and demographic factors, exposure to suicide behavior by others, chronic medical conditions, etc.\n","Supportive Nature: Take into account authors who are supporting other users in their writing.\n","\n","#Input to be summarised:\n","{posts}\\n\n","#Summary: [/INST]\n","\n","\"\"\""],"metadata":{"id":"bo1OLV_XPHEF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generate summaries"],"metadata":{"id":"lASaw8cUO6Sr"}},{"cell_type":"code","source":["def truncate_text(text, max_input_tokens=1800):\n","    tokens = tokenizer1.tokenize(text)\n","    if len(tokens) > max_input_tokens:\n","        truncated_tokens = tokens[:max_input_tokens]\n","        truncated_text = tokenizer1.convert_tokens_to_string(truncated_tokens)\n","        return truncated_text\n","    return text\n","\n","reserved_tokens = 512\n","\n","def extract_assistant_response(text):\n","    assistant_marker = \"[/INST]]]\"\n","    if assistant_marker in text:\n","        summary = text.split(assistant_marker)[1].strip()\n","    else:\n","        summary = text.strip()\n","    return summary\n","\n","\n","results = []\n","\n","for user_id in risk_supportiveness_df['userid'].unique()[:50]:\n","    full_text = data[data['userid'] == user_id]['combined_posts'].values[0]\n","\n","    meta_info = create_meta_info(user_id, risk_supportiveness_df, top_5_df)\n","    truncated_text = truncate_text(full_text, max_input_tokens=2048 - reserved_tokens)\n","\n","    input_text_zero_shot = Zero_prompt_model1.format(posts=truncated_text)\n","    input_text_zero_shot_meta = Zero_prompt_meta_model1.format(meta_info=meta_info, posts=truncated_text)\n","    input_text_cot = CoT_prompt_model1.format(posts=truncated_text)\n","    input_text_cot_meta = CoT_prompt_meta_model1.format(meta_info=meta_info, posts=truncated_text)\n","\n","    inputs_zero_shot = tokenizer1(input_text_zero_shot, return_tensors=\"pt\").to(\"cuda\")\n","    inputs_zero_shot_meta = tokenizer1(input_text_zero_shot_meta, return_tensors=\"pt\").to(\"cuda\")\n","    inputs_cot = tokenizer1(input_text_cot, return_tensors=\"pt\").to(\"cuda\")\n","    inputs_cot_meta = tokenizer1(input_text_cot_meta, return_tensors=\"pt\").to(\"cuda\")\n","\n","    outputs_zero_shot = model1.generate(**inputs_zero_shot, max_new_tokens=200)\n","    outputs_zero_shot_meta = model1.generate(**inputs_zero_shot_meta, max_new_tokens=200)\n","    outputs_cot = model1.generate(**inputs_cot, max_new_tokens=200)\n","    outputs_cot_meta = model1.generate(**inputs_cot_meta, max_new_tokens=200)\n","\n","    zero_shot_summary = extract_assistant_response(tokenizer1.decode(outputs_zero_shot[0], skip_special_tokens=True))\n","    zero_shot_meta_summary = extract_assistant_response(tokenizer1.decode(outputs_zero_shot_meta[0], skip_special_tokens=True))\n","    cot_summary = extract_assistant_response(tokenizer1.decode(outputs_cot[0], skip_special_tokens=True))\n","    cot_meta_summary = extract_assistant_response(tokenizer1.decode(outputs_cot_meta[0], skip_special_tokens=True))\n","\n","    results.append({\n","        \"userid\": user_id,\n","        \"zero_shot_summary\": zero_shot_summary,\n","        \"zero_shot_meta_summary\": zero_shot_meta_summary,\n","        \"cot_summary\": cot_summary,\n","        \"cot_meta_summary\": cot_meta_summary\n","    })\n","\n","results_df = pd.DataFrame(results)\n","results_df.to_csv('/content/drive/My Drive/Diss_Dataset/summary_comparisons2.csv', index=False)\n","\n","print(results_df.head())"],"metadata":{"id":"j_RD1TOv8pTy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Extract the summaries"],"metadata":{"id":"bwEaNdKfdqXq"}},{"cell_type":"code","source":["def extract_assistant_response(text):\n","    assistant_marker = \"Summary:\"\n","    if assistant_marker in text:\n","        summary = text.split(assistant_marker)[1].strip()\n","    else:\n","        summary = text.strip()\n","\n","    summary = summary.split('\\n')[0].strip()\n","\n","    return summary"],"metadata":{"id":"nGzUDl75vHCs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_df = pd.read_csv('/content/drive/My Drive/Diss_Dataset/summary_comparisons.csv')\n","print(results_df.head())\n","for index, row in results_df.iterrows():\n","    zero_shot_summary_stripped = extract_assistant_response(row['zero_shot_summary'])\n","    zero_shot_meta_summary_stripped = extract_assistant_response(row['zero_shot_meta_summary'])\n","    cot_summary_stripped = extract_assistant_response(row['cot_summary'])\n","    cot_meta_summary_stripped = extract_assistant_response(row['cot_meta_summary'])\n","\n","    results_df.at[index, 'zero_shot_summary'] = zero_shot_summary_stripped\n","    results_df.at[index, 'zero_shot_meta_summary'] = zero_shot_meta_summary_stripped\n","    results_df.at[index, 'cot_summary'] = cot_summary_stripped\n","    results_df.at[index, 'cot_meta_summary'] = cot_meta_summary_stripped\n","\n","print(results_df.head())\n","results_df.to_csv('/content/drive/My Drive/Diss_Dataset/summary_comparisons_cleaned.csv', index=False)\n"],"metadata":{"id":"G2nqecnPvI9c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["original_texts = []\n","zero_shot_summaries = []\n","zero_shot_meta_summaries = []\n","cot_summaries = []\n","cot_meta_summaries = []\n","\n","for index, row in results_df.iterrows():\n","    user_id = row['userid']\n","\n","    full_text = data[data['userid'] == user_id]['combined_posts'].values[0]\n","    original_texts.append(full_text)\n","\n","    zero_shot_summaries.append(row['zero_shot_summary'])\n","    zero_shot_meta_summaries.append(row['zero_shot_meta_summary'])\n","    cot_summaries.append(row['cot_summary'])\n","    cot_meta_summaries.append(row['cot_meta_summary'])"],"metadata":{"id":"ljolY5ZnIPIN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Define and produce evaluation metrics"],"metadata":{"id":"95Vvpj57dxe_"}},{"cell_type":"code","source":["model_sbert = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n","\n","def compute_sbert_similarity(summary, reference):\n","    summary_emb = model_sbert.encode(summary, convert_to_tensor=True)\n","    reference_emb = model_sbert.encode(reference, convert_to_tensor=True)\n","    similarity = util.pytorch_cos_sim(summary_emb, reference_emb)\n","    return similarity.item()"],"metadata":{"id":"Rezk0bBmkv4G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["qg_model = pipeline('text2text-generation', model='valhalla/t5-small-qg-hl')\n","qa_model = pipeline('question-answering')\n","\n","def generate_questions(text):\n","    inputs = f\"generate questions: {text}\"\n","    questions = qg_model(inputs)\n","    return [q['generated_text'] for q in questions]\n","\n","def answer_questions(context, questions):\n","    answers = []\n","    for question in questions:\n","        answer = qa_model(question=question, context=context)\n","        answers.append(answer['answer'])\n","    return answers\n","\n","def evaluate_with_qaeval(original_texts, summaries):\n","    qa_scores = []\n","    for original, summary in zip(original_texts, summaries):\n","        questions = generate_questions(original)\n","        original_answers = answer_questions(original, questions)\n","        summary_answers = answer_questions(summary, questions)\n","\n","        sbert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n","        similarities = [util.pytorch_cos_sim(sbert_model.encode(o), sbert_model.encode(s)).item()\n","                        for o, s in zip(original_answers, summary_answers)]\n","\n","        qa_scores.append(np.mean(similarities))\n","    return qa_scores\n","\n","zero_shot_qa_scores = evaluate_with_qaeval(original_texts, zero_shot_summaries)\n","zero_shot_meta_qa_scores = evaluate_with_qaeval(original_texts, zero_shot_meta_summaries)\n","cot_qa_scores = evaluate_with_qaeval(original_texts, cot_summaries)\n","cot_meta_qa_scores = evaluate_with_qaeval(original_texts, cot_meta_summaries)\n","\n","average_zero_shot = np.mean(zero_shot_qa_scores)\n","average_zero_shot_meta = np.mean(zero_shot_meta_qa_scores)\n","average_cot = np.mean(cot_qa_scores)\n","average_cot_meta = np.mean(cot_meta_qa_scores)\n","\n","print(f\"Zero-Shot Summaries - QA Scores: {average_zero_shot}\")\n","print(f\"Zero-Shot Meta Summaries - QA Scores: {average_zero_shot_meta}\")\n","print(f\"CoT Summaries - QA Scores: {average_cot}\")\n","print(f\"CoT Meta Summaries - QA Scores: {average_cot_meta}\")"],"metadata":{"id":"gy2DTur2mZV8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_with_bm25(original_texts, summaries):\n","    ir_scores = []\n","    for original, summary in zip(original_texts, summaries):\n","        original_sentences = sent_tokenize(original.lower())\n","        tokenized_sentences = [word_tokenize(s) for s in original_sentences]\n","\n","        bm25 = BM25Okapi(tokenized_sentences)\n","\n","        summary_tokens = word_tokenize(summary.lower())\n","\n","        scores = bm25.get_scores(summary_tokens)\n","\n","        ir_scores.append(sum(scores) / len(scores))\n","    return ir_scores\n","\n","zero_shot_ir_scores = evaluate_with_bm25(original_texts, zero_shot_summaries)\n","zero_shot_meta_ir_scores = evaluate_with_bm25(original_texts, zero_shot_meta_summaries)\n","cot_ir_scores = evaluate_with_bm25(original_texts, cot_summaries)\n","cot_meta_ir_scores = evaluate_with_bm25(original_texts, cot_meta_summaries)\n","\n","average_zero_shot_ir = mean(zero_shot_ir_scores)\n","average_zero_shot_meta_ir = mean(zero_shot_meta_ir_scores)\n","average_cot_ir = mean(cot_ir_scores)\n","average_cot_meta_ir = mean(cot_meta_ir_scores)\n","\n","print(f\"Average IR Score - Zero-Shot Summaries: {average_zero_shot_ir}\")\n","print(f\"Average IR Score - Zero-Shot Meta Summaries: {average_zero_shot_meta_ir}\")\n","print(f\"Average IR Score - CoT Summaries: {average_cot_ir}\")\n","print(f\"Average IR Score - CoT Meta Summaries: {average_cot_meta_ir}\")\n"],"metadata":{"id":"MaafHScQmIuT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_coverage(summary, reference):\n","    key_points = reference.split()[:10]\n","    coverage = sum(1 for kp in key_points if kp in summary) / len(key_points)\n","    return coverage"],"metadata":{"id":"3Ld0Ay1ImLhb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import precision_score\n","def compute_fidelity(summary, reference):\n","    key_points = reference.split()[:10]\n","    return precision_score([kp in summary for kp in key_points], [True] * len(key_points))\n"],"metadata":{"id":"8U1e7iX1mQRf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["original_texts = []\n","zero_shot_summaries = []\n","zero_shot_meta_summaries = []\n","cot_summaries = []\n","cot_meta_summaries = []\n","\n","for index, row in results_df.iterrows():\n","    user_id = row['userid']\n","    full_text = data[data['userid'] == user_id]['combined_posts'].values[0]\n","    original_texts.append(full_text)\n","\n","    zero_shot_summaries.append(row['zero_shot_summary'])\n","    zero_shot_meta_summaries.append(row['zero_shot_meta_summary'])\n","    cot_summaries.append(row['cot_summary'])\n","    cot_meta_summaries.append(row['cot_meta_summary'])\n","\n","for summary_type, summaries in [(\"zero_shot\", zero_shot_summaries),\n","                                (\"zero_shot_meta\", zero_shot_meta_summaries),\n","                                (\"cot\", cot_summaries),\n","                                (\"cot_meta\", cot_meta_summaries)]:\n","\n","    sbert_similarities = []\n","    coverage_scores = []\n","    fidelity_scores = []\n","\n","    for summary, reference in zip(summaries, original_texts):\n","        sbert_similarities.append(compute_sbert_similarity(summary, reference))\n","        coverage_scores.append(compute_coverage(summary, reference))\n","        fidelity_scores.append(compute_fidelity(summary, reference))\n","\n","    print(f\"{summary_type.capitalize()} Summaries -\")\n","    print(f\"Semantic Similarity (Sentence-BERT): {np.mean(sbert_similarities):.4f}\")\n","    print(f\"Coverage Score: {np.mean(coverage_scores):.4f}\")\n","    print(f\"Fidelity Score: {np.mean(fidelity_scores):.4f}\")"],"metadata":{"id":"y_IzdKP3mTP3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gc\n","torch.cuda.empty_cache()\n","gc.collect()\n","del model1\n","del tokenizer1"],"metadata":{"id":"wgs0WZ7vsDiw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name = \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\"\n","nli_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","nli_pipeline = pipeline(\"text-classification\", model=nli_model, tokenizer=tokenizer, return_all_scores=True)"],"metadata":{"id":"1QR095cbsebI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rouge = load_metric(\"rouge\")\n","bleu = load_metric(\"bleu\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IVK58mQVLKP3","executionInfo":{"status":"ok","timestamp":1725226137130,"user_tz":-60,"elapsed":1055,"user":{"displayName":"Aiden o'sullivan","userId":"05349309044784945157"}},"outputId":"bca40343-a3cf-4486-dadd-068d02076eb8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-35-ba51477253b7>:6: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","  rouge = load_metric(\"rouge\")\n"]}]},{"cell_type":"code","source":["def evaluate_nli(original_texts, summaries, nli_pipeline):\n","    consistency_scores = []\n","    contradiction_scores = []\n","\n","    for original, summary in tqdm(zip(original_texts, summaries), total=len(original_texts), desc=\"Evaluating NLI\"):\n","        input_pair = f\"{summary} [SEP] {original}\"\n","\n","        outputs = nli_pipeline(input_pair)\n","\n","        entailment_score = next(score['score'] for score in outputs[0] if score['label'].lower() == 'entailment')\n","        contradiction_score = next(score['score'] for score in outputs[0] if score['label'].lower() == 'contradiction')\n","\n","        consistency_scores.append(entailment_score)\n","        contradiction_scores.append(contradiction_score)\n","\n","    mean_consistency = sum(consistency_scores) / len(consistency_scores)\n","    max_contradiction = max(contradiction_scores)\n","\n","    return mean_consistency, max_contradiction"],"metadata":{"id":"qjQVjV6JslRH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_bertscore(original_texts, summaries):\n","    P, R, F1 = bert_score(summaries, original_texts, lang=\"en\", verbose=True)\n","    return P.mean().item(), R.mean().item(), F1.mean().item()\n","\n","def compute_rouge_bleu(original_texts, summaries):\n","    rouge_results = rouge.compute(predictions=summaries, references=original_texts)\n","    bleu_results = bleu.compute(predictions=[summary.split() for summary in summaries],\n","                                references=[[text.split()] for text in original_texts])\n","\n","    return rouge_results, bleu_results\n","\n","print(\"Computing BERTScore for Zero-Shot Summaries...\")\n","zero_shot_P, zero_shot_R, zero_shot_F1 = compute_bertscore(original_texts, zero_shot_summaries)\n","\n","print(\"Computing BERTScore for Zero-Shot Meta Summaries...\")\n","zero_shot_meta_P, zero_shot_meta_R, zero_shot_meta_F1 = compute_bertscore(original_texts, zero_shot_meta_summaries)\n","\n","print(\"Computing BERTScore for CoT Summaries...\")\n","cot_P, cot_R, cot_F1 = compute_bertscore(original_texts, cot_summaries)\n","\n","print(\"Computing BERTScore for CoT Meta Summaries...\")\n","cot_meta_P, cot_meta_R, cot_meta_F1 = compute_bertscore(original_texts, cot_meta_summaries)\n","\n","print(\"Computing ROUGE and BLEU for Zero-Shot Summaries...\")\n","zero_shot_rouge, zero_shot_bleu = compute_rouge_bleu(original_texts, zero_shot_summaries)\n","\n","print(\"Computing ROUGE and BLEU for Zero-Shot Meta Summaries...\")\n","zero_shot_meta_rouge, zero_shot_meta_bleu = compute_rouge_bleu(original_texts, zero_shot_meta_summaries)\n","\n","print(\"Computing ROUGE and BLEU for CoT Summaries...\")\n","cot_rouge, cot_bleu = compute_rouge_bleu(original_texts, cot_summaries)\n","\n","print(\"Computing ROUGE and BLEU for CoT Meta Summaries...\")\n","cot_meta_rouge, cot_meta_bleu = compute_rouge_bleu(original_texts, cot_meta_summaries)\n","\n","print(\"Evaluating NLI for Zero-Shot Summaries...\")\n","zero_shot_consistency, zero_shot_contradiction = evaluate_nli(original_texts, zero_shot_summaries, nli_pipeline)\n","\n","print(\"Evaluating NLI for Zero-Shot Meta Summaries...\")\n","zero_shot_meta_consistency, zero_shot_meta_contradiction = evaluate_nli(original_texts, zero_shot_meta_summaries, nli_pipeline)\n","\n","print(\"Evaluating NLI for CoT Summaries...\")\n","cot_consistency, cot_contradiction = evaluate_nli(original_texts, cot_summaries, nli_pipeline)\n","\n","print(\"Evaluating NLI for CoT Meta Summaries...\")\n","cot_meta_consistency, cot_meta_contradiction = evaluate_nli(original_texts, cot_meta_summaries, nli_pipeline)\n","\n","print(f\"Zero-Shot Summaries - BERTScore: P: {zero_shot_P:.4f}, R: {zero_shot_R:.4f}, F1: {zero_shot_F1:.4f}\")\n","print(f\"Zero-Shot Meta Summaries - BERTScore: P: {zero_shot_meta_P:.4f}, R: {zero_shot_meta_R:.4f}, F1: {zero_shot_meta_F1:.4f}\")\n","print(f\"CoT Summaries - BERTScore: P: {cot_P:.4f}, R: {cot_R:.4f}, F1: {cot_F1:.4f}\")\n","print(f\"CoT Meta Summaries - BERTScore: P: {cot_meta_P:.4f}, R: {cot_meta_R:.4f}, F1: {cot_meta_F1:.4f}\")\n","\n","print(f\"Zero-Shot Summaries - ROUGE: {zero_shot_rouge}, BLEU: {zero_shot_bleu['bleu']:.4f}\")\n","print(f\"Zero-Shot Meta Summaries - ROUGE: {zero_shot_meta_rouge}, BLEU: {zero_shot_meta_bleu['bleu']:.4f}\")\n","print(f\"CoT Summaries - ROUGE: {cot_rouge}, BLEU: {cot_bleu['bleu']:.4f}\")\n","print(f\"CoT Meta Summaries - ROUGE: {cot_meta_rouge}, BLEU: {cot_meta_bleu['bleu']:.4f}\")\n","\n","print(f\"Zero-Shot Summaries - NLI: Mean Consistency: {zero_shot_consistency:.4f}, Max Contradiction: {zero_shot_contradiction:.4f}\")\n","print(f\"Zero-Shot Meta Summaries - NLI: Mean Consistency: {zero_shot_meta_consistency:.4f}, Max Contradiction: {zero_shot_meta_contradiction:.4f}\")\n","print(f\"CoT Summaries - NLI: Mean Consistency: {cot_consistency:.4f}, Max Contradiction: {cot_contradiction:.4f}\")\n","print(f\"CoT Meta Summaries - NLI: Mean Consistency: {cot_meta_consistency:.4f}, Max Contradiction: {cot_meta_contradiction:.4f}\")"],"metadata":{"id":"nKkSdzPRsn-R"},"execution_count":null,"outputs":[]}]}